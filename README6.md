# Common LeetCode Patterns for Problem-Solving

Below is a comprehensive summary of common coding interview patterns (often seen on LeetCode), each with a clear definition, when to use it, and a brief example.

## Sliding Window

**Definition:** The *sliding window* pattern is an algorithmic technique where a subset of contiguous elements (a "window") moves through the data to solve problems involving subarrays or substrings. Instead of recomputing results from scratch for each new window position, the algorithm updates the result by removing the effect of the element leaving the window and adding the new element entering it ([Cracking the Coding Interview: Part 2 – The Sliding Window Pattern - DEV Community](https://dev.to/zzeroyzz/cracking-the-coding-interview-part-2-the-sliding-window-pattern-520d#:~:text=The%20Sliding%20Window%20pattern%20is,data%20to%20minimize%20unnecessary%20work)). This makes it efficient for computing sums, counts, or other properties of all windows of a given size.

**When to use:** Use sliding window for problems dealing with contiguous sequences (subarrays or substrings) where you need to find an optimal value (such as maximum/minimum sum or length) or meet a condition within a window. It applies when the problem involves a fixed window size (e.g. subarray of size *k*) or a dynamic window that can expand or shrink to satisfy a condition ([Cracking the Coding Interview: Part 2 – The Sliding Window Pattern - DEV Community](https://dev.to/zzeroyzz/cracking-the-coding-interview-part-2-the-sliding-window-pattern-520d#:~:text=,to%20Use%20Sliding%20Window)). Common scenarios include finding maximum sums, minimum lengths, or checking if a window meets a certain criterion in an array/string.

**Example:** *Maximum Sum Subarray of Size K:* Given an array of integers and an integer `k`, find the maximum sum of any contiguous subarray of length `k`. Using a sliding window, you can maintain the sum of the current window of size `k` and slide it across the array, updating the sum in O(1) time at each step. For instance, for array `[2, 1, 5, 1, 3, 2]` and `k = 3`, the windows are `[2,1,5]` (sum 8), `[1,5,1]` (sum 7), `[5,1,3]` (sum 9), `[1,3,2]` (sum 6), so the maximum sum is 9 ([Cracking the Coding Interview: Part 2 – The Sliding Window Pattern - DEV Community](https://dev.to/zzeroyzz/cracking-the-coding-interview-part-2-the-sliding-window-pattern-520d#:~:text=,k)). Sliding window is also used for variable-sized problems like finding the smallest subarray with sum ≥ S by expanding or contracting the window as needed.

## Two Pointers

**Definition:** The *two pointers* technique uses two indices (pointers) to iterate through a data structure (usually an array or linked list) in a coordinated way. Often the pointers start at different ends or positions and move towards each other or in some direction based on the problem logic ([Two-Pointer Technique, an In-Depth Guide: Concepts Explained | Questions to Try | Visuals and Animations : r/leetcode](https://www.reddit.com/r/leetcode/comments/18g9383/twopointer_technique_an_indepth_guide_concepts/#:~:text=The%20two,before%20meeting%20in%20the%20middle)). For example, one pointer might start at the beginning and another at the end of a sorted array, moving inward until they meet.

**When to use:** This technique is your go-to when the problem involves finding pairs (or triplets) in a sequence that meet certain criteria (such as a target sum or a particular order) ([Two-Pointer Technique, an In-Depth Guide: Concepts Explained | Questions to Try | Visuals and Animations : r/leetcode](https://www.reddit.com/r/leetcode/comments/18g9383/twopointer_technique_an_indepth_guide_concepts/#:~:text=This%20technique%20should%20be%20your,that%20meet%20a%20certain%20criteria)). It works best on sorted arrays or linked lists, or when the problem can be reduced to a sorted sequence, because the relative order of elements can guide how the pointers move. Common use-cases include: finding two numbers in a sorted array that add up to a target, removing duplicates from a sorted list, or partitioning arrays. It eliminates the need for nested loops by intelligently moving the pointers based on the current sum or comparison.

**Example:** *Pair with Target Sum (Two Sum II):* Given a sorted array and a target sum, find two numbers that add up to the target. Using two pointers, one at the start and one at the end of the array, you can check the sum of the pointed elements and adjust pointers: if the sum is too large, move the right pointer left; if too small, move the left pointer right. For instance, in a sorted array `[1, 3, 4, 6, 8, 10, 13]` and target `13`, start with left=1 and right=13 (sum 14, too high) then move the right pointer left, etc., until you find the pair `3` and `10` which sum to 13 ([Two-Pointer Technique, an In-Depth Guide: Concepts Explained | Questions to Try | Visuals and Animations : r/leetcode](https://www.reddit.com/r/leetcode/comments/18g9383/twopointer_technique_an_indepth_guide_concepts/#:~:text=Starting%20with%20a%20sorted%20array,sum%20to%20the%20given%20target)). This finds the pair in O(n) time instead of O(n²) brute force.

## Fast & Slow Pointers (Tortoise and Hare)

**Definition:** The *fast and slow pointers* pattern involves two pointers iterating through a structure at different speeds – typically, the "fast" pointer moves two steps at a time and the "slow" pointer moves one step. This approach is famously used in Floyd’s cycle detection algorithm. By moving at different speeds, the two pointers may eventually meet inside a cycle if one exists ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Description%3A%20In%20this%20method%2C%20two,speeds%20in%20a%20data%20structure)).

**When to use:** Use fast & slow pointers for problems dealing with cyclic structures or where you need to find a midpoint or cycle. Common applications are detecting a cycle in a linked list (or array sequence), finding the starting point of a loop, or finding the middle of a linked list in one pass ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Description%3A%20In%20this%20method%2C%20two,speeds%20in%20a%20data%20structure)). This pattern is particularly effective in linked list problems (cycle detection, palindrome check by finding mid, etc.) and sometimes in array problems where a cycle occurs in indexing.

**Example:** *Cycle Detection in Linked List:* To determine if a linked list has a loop, initialize one pointer `slow` and one pointer `fast` at the head. Move `slow` one node and `fast` two nodes at a time. If there is a cycle, eventually `fast` will catch up to `slow` inside the loop, indicating a cycle ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Description%3A%20In%20this%20method%2C%20two,speeds%20in%20a%20data%20structure)). If `fast` reaches the end (null) without meeting `slow`, then there is no cycle. This is Floyd's Cycle Detection (Tortoise & Hare) algorithm in action. Another example is finding the **middle** of a linked list: if `fast` moves twice as quickly as `slow`, when `fast` reaches the end, `slow` will be at the middle.

## Merge Intervals

**Definition:** The *merge intervals* pattern deals with intervals (ranges with start and end) and focuses on merging overlapping intervals into larger ones. The typical approach is to sort the intervals by starting time, then iterate and merge any intervals that overlap (i.e., when one interval’s start is within the previous interval’s range) ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Description%3A%20This%20pattern%20involves%20merging,overlapping%20intervals)).

**When to use:** Use this pattern for problems involving time ranges or numeric intervals where you need to combine overlapping ranges or find gaps. Scenarios include merging meeting times, simplifying ranges, finding intersections or unions of intervals, or scheduling problems. Essentially, whenever you have a list of intervals and the problem is about combining or comparing them (for example, checking free time or covering a range), merging after sorting is the key approach ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Description%3A%20This%20pattern%20involves%20merging,overlapping%20intervals)).

**Example:** *Merge Overlapping Intervals:* Given intervals such as `[1,3], [2,6], [8,10], [15,18]`, first sort them by start time (they’re already sorted in this example). Then, merge as you iterate: `[1,3]` and `[2,6]` overlap (since 2 ≤ 3), so merge them into `[1,6]`. The next interval `[8,10]` does not overlap with `[1,6]` (8 > 6), so it stays separate. `[15,18]` stands alone as well. The result after merging is `[1,6], [8,10], [15,18]` ([Mastering Merge Interval Patterns: A Guide to Solving Interval-Based Problems | by Architect Algos | ArchitectAlgos](https://www.architectalgos.com/mastering-merge-interval-patterns-a-guide-to-solving-interval-based-problems-1a14ef065876#:~:text=Example%3A%20Consider%20the%20intervals%20%60,15%2C%2018)). This pattern underlies many interval problems like inserting a new interval into a set of non-overlapping intervals, or finding interval intersections.

## Cyclic Sort

**Definition:** The *cyclic sort* pattern is an in-place sorting strategy for situations where array elements are in a known range (typically 1 to *n* or 0 to *n*). The idea is to place each element at its correct index by swapping elements along cycles until each number is in its proper position ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=6)). It’s optimal in terms of number of writes, effectively arranging the array by repeatedly swapping mismatched elements into their correct locations.

**When to use:** Use cyclic sort for problems involving arrays of integers where the numbers fall within a continuous range (for example 1 to *n*). It's particularly useful for finding missing or duplicate numbers in such arrays. By sorting the array in O(n) time using swaps, you can then easily spot discrepancies (like a number out of place indicating a duplicate or a position that never got the correct number indicating a missing number) ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Description%3A%20This%20pattern%20involves%20sorting,numbers%20in%20a%20given%20range)). Common problems using this pattern include: *Find the Missing Number*, *Find All Duplicates in an Array*, *Find the Smallest Missing Positive*, etc., all of which are easier after a cyclic sort.

**Example:** *Sorting 1 to N:* Suppose you have an array of `n` numbers, where each number is in the range 1 to *n* (with one missing number if length is n, or one duplicate, etc.). For example: `arr = [2, 6, 4, 3, 1, 5]` where the range is 1–6. Cyclic sort will iterate through the array and swap elements to their correct positions. We see `2` is at index 0 but should be at index 1 (since indices start at 0, index = number-1 for range 1–n). Swap `2` with whatever is at index 1. Continue this process: the array will become sorted as `[1, 2, 3, 4, 5, 6]` with each element put in its rightful place ([Coding Pattern: Cyclic Sort. Cycle sort is an in-place sorting… | by Fernando Salas | Stackademic](https://blog.stackademic.com/coding-pattern-cyclic-sort-96511b0f60ac#:~:text=Input%3A%20,1%2C%205)). After this procedure, if any index i doesn’t have the value i+1, that value is missing (or any extras are duplicates), depending on the problem. Cyclic sort thus sorts the array in linear time and sets up easy detection of anomalies.

## In-place Reversal of a Linked List

**Definition:** This pattern involves reversing a linked list (or part of it) **in place**, meaning the links between nodes are changed without using extra space for another list. We iteratively reverse pointers such that each node’s `next` pointer is flipped to point to its previous node, all within the original list structure ([Coding Patterns: In-place Reversal of a Linked List - emre.me](https://emre.me/coding-patterns/in-place-reversal-of-a-linked-list/#:~:text=We%20will%20reverse%20the%20,node%20that%20we%20have%20processed)). By the end, the head of the list becomes the tail and vice versa, effectively reversing the list.

**When to use:** Apply in-place reversal for problems where you need to reverse all or part of a linked list without using additional memory. This is common in scenarios such as reversing the entire list, reversing a sub-section of the list (between two indices), or reversing nodes in groups of *k*. The pattern is useful whenever a problem asks for a modified linked list order (especially reversed order) and specifically mentions doing it in-place or in one pass. It’s also a step in certain algorithms (e.g., many solutions for reorder-list or adding numbers represented by linked lists use partial reversals).

**Example:** *Reverse a Linked List:* Given a singly linked list `1 → 2 → 3 → 4 → 5 → NULL`, we want to reverse it. We maintain two pointers: `previous` and `current`. Initially, `previous = NULL` and `current = head`. Traverse the list, and for each node, redirect its `next` pointer to the `previous` node (this is the in-place reversal step) ([Coding Patterns: In-place Reversal of a Linked List - emre.me](https://emre.me/coding-patterns/in-place-reversal-of-a-linked-list/#:~:text=We%20will%20reverse%20the%20,node%20that%20we%20have%20processed)), then move `previous` and `current` one step forward. After processing all nodes, `previous` will point to the new head (the original tail). The list becomes `5 → 4 → 3 → 2 → 1 → NULL` ([Coding Patterns: In-place Reversal of a Linked List - emre.me](https://emre.me/coding-patterns/in-place-reversal-of-a-linked-list/#:~:text=Example%3A)). Variations of this example include reversing a portion of the list (between two given positions) or reversing nodes in chunks (e.g., reversing every 3 nodes in the list).

## Breadth-First Search (BFS)

**Definition:** *Breadth-first search (BFS)* is an algorithm for traversing or searching through tree or graph data structures level by level. It uses a queue to keep track of the next nodes to visit. Starting from a source node (or root of a tree), BFS visits all immediate neighbors first, then moves on to the neighbors of those neighbors, and so on, expanding outward layer by layer ([Breadth-first search - Wikipedia](https://en.wikipedia.org/wiki/Breadth-first_search#:~:text=Breadth,encountered%20but%20not%20yet%20explored)). This results in visiting nodes in order of increasing distance (or depth) from the start node.

**When to use:** Use BFS when you need to traverse a graph or tree in layers, or find the shortest path in an unweighted graph. It’s particularly useful for scenarios like finding the minimum number of steps to reach a target (since BFS finds the shortest path in terms of edge count), doing a level-order traversal of a tree (visiting nodes by depth), or any scenario where exploring neighbors first is advantageous (e.g., finding the closest occurrence of something). For example, BFS is used in finding the shortest path in a maze or grid (where each move is one step), in networking to find the shortest route, or in puzzle solving (like word ladders) to find the minimum transformation count.

**Example:** *Level Order Traversal (Binary Tree BFS):* Given a binary tree, BFS can print nodes level by level. Starting at the root, visit all nodes at depth 1, then depth 2, etc. For a tree like:
```
    1
   / \
  2   3
 / \   \
4   5   6
``` 
BFS traversal yields `1, 2, 3, 4, 5, 6` (grouped by level). We use a queue: enqueue root (1), then dequeue it to visit, enqueue its children (2,3), then visit 2 and 3, enqueue their children (4,5,6), and so on ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Description%3A%20This%20pattern%20involves%20level,traversal%20of%20a%20tree)). In a grid, BFS could start at a source cell and explore outward, ensuring the first time you reach a new cell is the shortest path. BFS guarantees the first time you encounter a target node, you have found the shortest path to it (in terms of number of edges). 

## Depth-First Search (DFS)

**Definition:** *Depth-first search (DFS)* is an algorithm for traversing or searching tree and graph structures by exploring as far down one path as possible before backtracking. Starting from a root node (or an arbitrary start node in a graph), DFS goes deep into one branch (following children or adjacent nodes) until it cannot continue, then backtracks to explore other branches. It typically uses a stack data structure (either implicitly via recursion or explicitly) to remember to return to previous nodes ([Depth-first search - Wikipedia](https://en.wikipedia.org/wiki/Depth-first_search#:~:text=Depth,track%20of%20the%20nodes%20discovered)).

**When to use:** Use DFS when you need to explore all paths or possibilities, or when you want to reach deep nodes quickly. It's useful in scenarios like pathfinding when the solution might be far from the start and you want to search depth-wise, traversing a tree (preorder, inorder, postorder traversals are DFS variants), or solving puzzles and backtracking problems. DFS is also used to find connected components in a graph, to perform topological sorting (by exploring deeper before others), and in algorithms like solving mazes or labyrinths (where you go deep and backtrack upon hitting dead ends). In comparison to BFS, DFS is not guaranteed to find the shortest path, but it uses less memory and can be easier to implement recursively.

**Example:** *DFS Tree Traversal:* Consider a binary tree. A DFS preorder traversal would visit the root, then recursively visit the left subtree, then the right subtree. For the tree:
```
    A
   / \
  B   C
 / \
D   E
``` 
A DFS preorder yields A → B → D → E → C (visiting deep into B’s branch first, then backtracking to C). In a graph context, DFS can be used to find if there is a path between two nodes by exploring one route fully before trying another. Another example: solving a maze by taking a path and following it until you hit a dead end, then backtracking – this is DFS. The algorithm explores as far as possible along each branch (path) before backtracking ([Depth-first search - Wikipedia](https://en.wikipedia.org/wiki/Depth-first_search#:~:text=Depth,track%20of%20the%20nodes%20discovered)).

## Backtracking

**Definition:** *Backtracking* is a general algorithmic approach where you build a solution incrementally and abandon (“backtrack” from) a path as soon as you determine it cannot lead to a valid solution. Essentially, it involves trying all possible options (DFS through the solution space) and undoing (“backtracking”) choices that do not meet the problem’s constraints ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=15)). It’s often implemented via recursion, exploring one possibility, then recursing back to try another.

**When to use:** Use backtracking for problems that require searching through a large solution space of combinations or permutations where you need to find *all* solutions or a particular solution subject to constraints. Classic cases include puzzles and combinatorial problems: solving Sudoku, N-Queens, generating all subsets or permutations, solving crosswords or word search, and combination sum problems. Whenever you can construct a solution step by step and have to validate partial solutions and potentially discard them, backtracking is an apt approach. It systematically searches all possibilities but prunes large swathes of them by early detection of invalid paths (thus saving time).

**Example:** *N-Queens Problem:* Place 8 queens on a chessboard so that none attack each other. A backtracking solution will place a queen on row 1 in some column, then row 2, and so on. If at some row you find no valid column (every column is attacked by an already placed queen), you backtrack to the previous row and move the queen there to the next possible column, then continue forward again. This way, you explore all placements systematically ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Description%3A%20This%20pattern%20involves%20exploring,you%27re%20on%20the%20wrong%20path)). Another example is solving a **Sudoku** puzzle: fill digits one cell at a time, and if you reach a contradiction (two of the same number in a row/col/box), backtrack to change the previous placements ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Usage%3A%20It%27s%20typically%20used%20for,combinatorial%20problems%2C%20puzzles%2C%20and%20games)). Backtracking ensures you eventually find all valid solutions (or one solution, depending on the task) by exploring the search space exhaustively but efficiently.

## Dynamic Programming (DP)

**Definition:** *Dynamic programming* is an algorithmic paradigm that solves complex problems by breaking them down into simpler overlapping subproblems and solving each subproblem just once. The key idea is to store the results of subproblems (memoization or tabulation) so that when the same subproblem arises, one can reuse the computed result instead of recomputing it ([Introduction to Dynamic Programming](https://developerinsider.co/introduction-to-dynamic-programming/#:~:text=Dynamic%20Programming%20is%20an%20algorithmic,computing%20the%20same%20results%20again)). This often involves defining a state that represents a subproblem and a recurrence (or relation) that builds the solution of larger subproblems from smaller ones.

**When to use:** Use dynamic programming when a problem has **overlapping subproblems** (the same smaller subproblems are solved multiple times in naive recursion) and **optimal substructure** (an optimal solution to the problem can be composed from optimal solutions of its subproblems). Typical signs include recursive brute-force solutions with repeated computations (e.g., Fibonacci, where `F(n) = F(n-1)+F(n-2)` leads to repeated calculation of lower terms) or choices that require evaluating all possibilities and taking the best. DP is common in optimization problems (maximize or minimize something) and counting problems. Classic examples are: computing Fibonacci numbers efficiently, 0/1 Knapsack (choose items with max value under weight limit), coin change (count ways to make change), longest common subsequence, edit distance, etc. If you can formulate a problem in terms of smaller problems (often via recursion) and there’s overlapping, DP will greatly improve efficiency by avoiding redundant work ([Introduction to Dynamic Programming](https://developerinsider.co/introduction-to-dynamic-programming/#:~:text=Dynamic%20Programming%20is%20an%20algorithmic,computing%20the%20same%20results%20again)).

**Example:** *0/1 Knapsack:* You have a set of items, each with a weight and value, and a weight capacity. The goal is to pick items with maximum total value without exceeding the capacity. A DP solution would create a table `dp[i][w]` representing the maximum value achievable with the first `i` items and capacity `w`. Using recurrence relations (include or exclude item i), you fill this table to find the answer for `dp[n][W]`. Each subproblem (a certain capacity with a subset of items) is solved once, building up to the full solution. Another example is the *Fibonacci sequence*: a naive recursion recalculates F(n) many times; using DP (memoization), you store results for F(1), F(2), ..., and reuse them to get F(n) in linear time. As a simpler example, consider *Unique Paths*: counting paths in a grid from top-left to bottom-right. You can use DP by noting `paths[i][j] = paths[i-1][j] + paths[i][j-1]` (coming from above or left). By solving all sub-grid counts up to that point, you efficiently get the total paths. These problems, and others like *Equal Subset Sum Partition* or *Subset Sum*, are textbook cases for DP ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Description%3A%20This%20pattern%20deals%20with,maximum%20value%20we%20can%20carry)).

## Topological Sort (Graph)

**Definition:** *Topological sort* is an algorithm for ordering the vertices of a directed acyclic graph (DAG) such that for every directed edge from U to V, U comes before V in the ordering ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=17)). In other words, it produces a linear sequence of nodes that respects all prerequisite relationships (no node appears before any of its predecessors). The typical implementation uses DFS (with recursion stack to detect cycles and post-order traversal) or BFS (Kahn’s algorithm using in-degrees).

**When to use:** Use topological sort when you have dependency ordering problems. This pattern is applicable to any scenario where certain tasks must be done before others (prerequisites). Common examples: course scheduling (some courses depend on others), task scheduling in a build system (some jobs depend on outputs of others), resolving package dependencies. Essentially, when you can model the problem as a directed graph of dependencies and need an order to complete all tasks, topological sorting is the way to find a valid sequence ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Description%3A%20This%20pattern%20involves%20sorting,comes%20before%20the%20following%20node)). Note that a topological order exists only if the graph has no cycles (i.e., it’s a DAG). If there’s a cycle, no valid ordering exists (which is often an important check in these problems).

**Example:** *Course Schedule:* Suppose there are courses 0,1,2,3 and prerequisites: 0 → 1 (take 0 before 1), 0 → 2, 1 → 3, 2 → 3. We can build a graph and topologically sort it. One valid order is 0 → 2 → 1 → 3, which respects all prerequisites (0 comes before 1 and 2, and both 1 and 2 come before 3). A BFS-based approach (Kahn’s algorithm) would start with nodes that have no prerequisites (here, 0), then remove it and reduce indegrees of neighbors (1 and 2). Once their indegree drops to zero, they become available (say we then take 1 and 2, order could vary), and finally 3. This yields an order like [0, 2, 1, 3] which is a topological sort. Another example is the *Alien Dictionary* problem: given words in an alien language sorted lexicographically, determine the order of characters – this can be solved by building a graph of character dependencies and topologically sorting it ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Usage%3A%20It%27s%20used%20for%20scheduling,on%20how%20you%20process%20nodes)).

## Binary Search

**Definition:** *Binary search* is a divide-and-conquer algorithm to find a target value in a **sorted** sequence (array or list) by repeatedly dividing the search interval in half. Starting with the entire range, it compares the target to the middle element: if the target is smaller, it eliminates the upper half; if larger, it eliminates the lower half. By halving the range each time, binary search locates the target (if it exists) with O(log n) comparisons ([10.1 Binary search - Hello Algo](https://www.hello-algo.com/en/chapter_searching/binary_search/#:~:text=Binary%20search%20is%20an%20efficient,the%20search%20interval%20becomes%20empty)).

**When to use:** Use binary search when you have a sorted array or list (or you can define a monotonic condition over an index range) and you need to find the position of a target element, or decide if it exists. It’s also used in more abstract scenarios, like finding the smallest value that satisfies a condition (the "binary search on answer" technique), provided the condition function is monotonic. Typical uses: looking up an element in a sorted array, finding the insert position (like lower_bound/upper_bound), searching in a sorted rotated array (by modifying the condition), finding a threshold (e.g., the minimum capacity to ship packages in D days, where feasibility is monotonic). The requirement is that the search space can be halved based on comparisons, which usually means sorted input or monotonic behavior.

**Example:** *Search in Sorted Array:* Given a sorted array `nums = [1, 3, 5, 7, 9, 12]` and a target `7`, binary search will find 7 efficiently. Check the middle element (for indices 0–5, mid is index 2 with value 5). Since 7 > 5, discard the left half [1,3,5] and focus on [7,9,12]. Now mid of [7,9,12] is 9 (index 4 overall). 7 < 9, so discard the right half of this segment (which is [9,12]), leaving just [7]. The middle (and only) element is 7, which matches the target. In three comparisons we found the target, whereas linear search would take six in the worst case. In general, binary search repeatedly halves the search interval until it either finds the target or the interval is empty ([10.1 Binary search - Hello Algo](https://www.hello-algo.com/en/chapter_searching/binary_search/#:~:text=Binary%20search%20is%20an%20efficient,the%20search%20interval%20becomes%20empty)) ([10.1 Binary search - Hello Algo](https://www.hello-algo.com/en/chapter_searching/binary_search/#:~:text=Question)). Another example is finding the square root of a number *N*: you can binary search the result between 0 and N (or 0 and N/2) by squaring midpoints to compare to N. This uses the principle that if mid² is too low, you go higher; if too high, go lower.

## Bit Manipulation

**Definition:** *Bit manipulation* refers to techniques that directly operate on the binary representation of numbers (bits). It uses bitwise operators like AND (`&`), OR (`|`), XOR (`^`), NOT (`~`), and bit shifts (`<<`, `>>`) to efficiently perform operations that would be cumbersome or slower with arithmetic or loops. Bit manipulation patterns often exploit properties of binary (like XOR to cancel out identical bits) to achieve results in constant time ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=14)).

**When to use:** Use bit manipulation when solving problems that involve flags, subsets, or require optimizing operations on integers. Common scenarios include: toggling or checking specific bits in a bitmask (for example, using an integer’s bits to represent a subset of a set), optimizing multiplication/division by powers of two (using shifts), checking for even/odd (via the least significant bit), swapping values without extra space (via XOR), and problems like finding a unique element among duplicates (XOR accumulation). Bit tricks can also be used to check if a number is a power of two (`n & (n-1) == 0` is true only for powers of two) or to count the number of 1 bits (`n & (n-1)` repeatedly drops the lowest set bit). They’re favored in performance-critical code and for certain algorithmic puzzles where arithmetic approaches are less direct.

**Example:** *Single Number (XOR trick):* You have an array where every number appears twice except one number which appears only once. To find the unique number, you can XOR all the elements together. Pairs of identical numbers will cancel out because `x ^ x = 0` and `0 ^ y = y`. The result of XORing all numbers will be the single number that was unpaired ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Usage%3A%20It%27s%20used%20when%20we,manipulate%20and%20compare%20bits%20directly)). For instance, `[2, 5, 2, 3, 5]` → `(2^2) ^ (5^5) ^ 3 = 0 ^ 0 ^ 3 = 3`. Another example: using a bitmask to represent a subset of {A, B, C, D} with a 4-bit number (e.g., `0101` represents {B, D}). You can iterate through all subsets of a set of size *n* by counting from `0` to `(1<<n)-1` and interpreting each number’s binary as inclusion/exclusion of elements. Bit manipulation is also used in *bit DP* techniques or optimizing certain DP states by encoding them as bits.

## Trie (Prefix Tree)

**Definition:** A *Trie* (pronounced "try"), or prefix tree, is a tree-based data structure that stores a set of strings by breaking them into characters, with one node (or edge) per character. Each path down the tree represents a prefix of some of the stored keys. This allows efficient insertion and lookup of strings by their prefixes ([Basic Topics | LeetCode The Hard Way](http://leetcodethehardway.com/tutorials/category/basic-topics#:~:text=in%20array%20or%20string,25Next%20Arrays)). The root represents an empty prefix, and each node can have multiple children (for each possible character). A special end-of-word marker is often used to denote completed words in the trie.

**When to use:** Use a trie when you need to store and retrieve strings (or similar sequences) efficiently, especially for prefix-based queries. Tries are particularly useful for autocomplete systems (finding all words with a given prefix), spell checkers (finding suggestions with common prefixes), IP routing (longest prefix matching), or any scenario where prefix search or incremental lookup is required. They trade space for speed: tries can use more memory than a simple list of words, but they allow querying by prefix in O(m) time (where *m* is the length of the query string), independently of the number of stored strings. If you need to support many lookups of keys or prefixes, or dynamic insertion of keys with quick prefix queries, a trie is appropriate.

**Example:** *Autocomplete System:* Suppose you have a dictionary of words: {"cat", "cap", "cape", "bat"}. A trie for these will have a root, a branch for 'c' and one for 'b'. Under the 'c' branch, there is an 'a' node, then a 't' node marking the end of "cat", and a parallel path 'a' → 'p' → (end) for "cap", with an extension from 'p' → 'e' → (end) for "cape". If a user types "ca", you can traverse the trie following 'c' → 'a' and from that node, gather all descendants that mark word endings, yielding suggestions ["cat", "cap", "cape"]. Tries are also used in implementing *word search* puzzles and *IP routing*: for instance, storing binary representations of IP addresses in a trie allows longest prefix matching for routing. In summary, a trie enables efficient prefix lookup — it's *most useful for looking up words by prefix* quickly ([Search: trie](https://sudonull.com/?q=trie#:~:text=A%20trie%20stores%20a%20set,look%20up%20words%20by%20prefix)).

## Heap (Priority Queue)

**Definition:** A *heap* is a specialized tree-based data structure that satisfies the **heap property**: in a max-heap, every parent node’s value is greater than or equal to the values of its children (in a min-heap, every parent is less than or equal to its children). This property ensures that the largest (for max-heap) or smallest (for min-heap) element is always at the root of the tree ([data structures - When would I want to use a heap? - Stack Overflow](https://stackoverflow.com/questions/749199/when-would-i-want-to-use-a-heap#:~:text=A%20heap%20is%20a%20tree,any%20of%20its%20descendant%20nodes)). Heaps are typically implemented with arrays (for a binary heap, the children of index *i* are at indices 2*i*+1 and 2*i*+2). A common interface for heaps is the **priority queue**, which supports operations like insert, remove-max/min, and peek-max/min efficiently.

**When to use:** Use a heap when you need quick access to the highest or lowest priority element in a collection ([data structures - When would I want to use a heap? - Stack Overflow](https://stackoverflow.com/questions/749199/when-would-i-want-to-use-a-heap#:~:text=157)). This makes heaps ideal for job scheduling (always pick next task with highest priority or earliest deadline), for implementing Dijkstra’s shortest path (min-heap for picking the smallest distance node), for any “top K” problems (maintaining a min-heap of size K for largest K elements), and in simulation or merging scenarios (like merging k sorted lists, where a min-heap quickly gives the next smallest element). If you find yourself repeatedly needing to get the min or max element while also inserting or removing elements, a heap is the appropriate data structure. In summary, heaps are useful for **priority queues, schedulers, and anytime you need to continuously extract the min or max** efficiently ([data structures - When would I want to use a heap? - Stack Overflow](https://stackoverflow.com/questions/749199/when-would-i-want-to-use-a-heap#:~:text=However%2C%20the%20remainder%20of%20the,have%20access%20to%20the%20earliest%2Fbiggest)).

**Example:** *Find the Kth Largest Element:* To find the 3rd largest element in an array, you can maintain a min-heap of size 3. As you iterate through the array, insert elements into the heap. If the heap size exceeds 3, remove the smallest element. In the end, the heap will contain the 3 largest elements, and the root (smallest in the heap of 3) will be the 3rd largest overall. Each insertion/removal is O(log n), making the approach efficient. Another example: *Merge K Sorted Lists:* put the head of each list into a min-heap (by value). Repeatedly extract the smallest item (which is the next in sorted order overall) and then insert its next node into the heap. This produces a sorted merge in O(n log k) time. Heaps are also used in *heap sort*: repeatedly extract the max to sort an array in ascending order. In all these cases, the heap property ensures that extracting the next highest/lowest is quick (O(1) for peek, O(log n) for extract) and the rest of the elements remain partially ordered for subsequent operations ([data structures - When would I want to use a heap? - Stack Overflow](https://stackoverflow.com/questions/749199/when-would-i-want-to-use-a-heap#:~:text=157)).

## Monotonic Stack / Queue

**Definition:** A *monotonic stack* is a stack data structure that maintains its elements in sorted order (either non-increasing or non-decreasing) with respect to some property. As you push or pop elements, you ensure the stack’s values are always monotonic. Typically, a **monotonic increasing stack** holds elements so that each is less than or equal to the one below it (top is smallest), and a **monotonic decreasing stack** holds elements so that each is greater than or equal to the one below it (top is largest) ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=19)). A monotonic queue is similar but for a queue (often implemented with a deque), ensuring the elements in the deque are in sorted order.

**When to use:** Use a monotonic stack or queue for problems where you need to efficiently find the *next greater or smaller element* for each element in a sequence, or maintain a windowed maximum/minimum. Common problems include *Next Greater Element*, *Next Smaller Element*, *Stock Span*, *Trapping Rain Water*, and *Largest Rectangle in Histogram*. The monotonic stack provides a way to solve these in linear time by leveraging the order: as you iterate, you pop from the stack until the monotonic condition is restored, which effectively gives you relationships between the current element and those in the stack (like identifying the next greater element when you pop a smaller one) ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Description%3A%20This%20pattern%20involves%20using,decreasing%29%20order%20of%20elements)). A monotonic queue (often implemented with a deque) is very useful for the "sliding window maximum/minimum" problem: as you slide a window, you can push new elements and pop elements from the back that are smaller (for max) so that the front always holds the maximum in O(1) time per operation.

**Example:** *Next Greater Element:* Given an array, for each element find the next element to its right that is greater. Using a monotonic **decreasing** stack (where top of stack is the **first smaller** element), you iterate from left to right. Keep the stack of indices whose values are in decreasing order. For each new element, while the stack is not empty and the current element is greater than the value at the stack’s top index, pop the stack – the current element is the "next greater" for the popped index. Then push the current element’s index on the stack. This yields the result in O(n) time. For instance, for `[2, 5, 1, 3]`, the stack method finds next greater elements as `[5, -1, 3, -1]` (here -1 indicates no greater element). Another example: *Sliding Window Maximum:* to find the max in every window of size k, use a deque that holds indices and maintains a decreasing order of values. As you move the window, pop indices from the back while their values are less than the new entering value (to maintain order), and pop from the front if they move out of the window. The front of the deque always points to the max of the current window. This is an O(n) solution using a monotonic queue, far more efficient than the naive O(n*k) approach.

## Union Find (Disjoint Set Union)

**Definition:** *Union-Find*, or *Disjoint Set Union (DSU)*, is a data structure that tracks a set of elements partitioned into a number of disjoint (non-overlapping) subsets. It supports two operations efficiently: **find** (determining which subset a particular element belongs to, often returning a representative of that set) and **union** (merging two subsets into one). Typically implemented with parent pointers and union-by-rank with path compression, it can perform find/union in nearly constant amortized time ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=21)).

**When to use:** Use Union-Find in problems that involve connectivity, grouping, or component identification. If the problem asks “are these elements connected?” or “how many groups are formed by these connections?”, union-find is a strong choice. Common applications include: checking if adding an edge creates a cycle in a graph (used in Kruskal’s algorithm for MST), determining connected components in an undirected graph, solving networking connectivity queries, percolation problems, and puzzles like *connecting islands*. It’s also used in more advanced scenarios like image processing (grouping pixels into regions) or finding redundancies (redundant connection in a graph). Essentially, whenever you have incremental connection information and need to quickly answer connectivity queries, union-find is ideal.

**Example:** *Connecting Graph Components:* Imagine a scenario with `n` nodes and a list of undirected edges that connect some pairs of nodes. We can use union-find to determine how many connected components the graph has. Initially, each node is its own component (makeSet for each). For each edge (u, v), do `union(u, v)` to connect their components. After processing all edges, you can count the number of distinct parent representatives (using `find` for each node). For instance, with 5 nodes and edges `{0-1, 1-2, 3-4}`, union-find will form sets {0,1,2} and {3,4}, so 2 components remain. Union-find can also answer queries like “are 0 and 2 connected?” (Yes, since find(0) == find(2)), or find a *redundant connection* (an edge that connects two already-connected components) ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Usage%3A%20This%20pattern%20is%20particularly,in%20a%20graph%20or%20tree)). Another example: *Krusal’s MST algorithm* uses union-find to ensure no cycle is formed when picking the next smallest edge – if an edge connects two vertices already in the same set, it’s discarded. Union-find excels at these connectivity checks.

## Kadane’s Algorithm (Maximum Subarray)

**Definition:** *Kadane’s algorithm* is a dynamic programming approach that efficiently finds the maximum sum of a contiguous subarray within a one-dimensional array of numbers. It runs in linear time by iterating through the array and at each position computing the maximum subarray sum ending at that position, using the recurrence relation: `current_max = max(arr[i], current_max + arr[i])`. The global maximum subarray sum is tracked along the way ([Basic Topics | LeetCode The Hard Way](http://leetcodethehardway.com/tutorials/category/basic-topics#:~:text=solution%20to%20the%20problem%20%EF%B8%8F,Linear%20search%20is%20a%20searching)).

**When to use:** Use Kadane’s algorithm for problems asking for the largest sum of a contiguous subsequence. The classic use is the “Maximum Subarray” problem (given an array, find the contiguous subarray with the largest sum). Kadane’s can be extended or modified for variations like maximum product subarray (with some tweaks), or to also retrieve the subarray indices. Essentially, any time you have to find an optimal contiguous segment (max sum, or min sum by inverting signs), Kadane’s linear scan approach is applicable. It’s a specific case of DP where the subproblem is “best subarray ending here.” It can also be used in two dimensions (maximum sum submatrix) by reducing the 2D problem to repeated 1D Kadane applications.

**Example:** *Maximum Subarray:* Given an array `nums = [-2, 1, -3, 4, -1, 2, 1, -5, 4]`, Kadane’s algorithm will find the maximum sum contiguous subarray which is `[4, -1, 2, 1]` with sum 6 ([30 Top Dynamic Programming Interview Questions and Answers (2024)](https://www.withoutbook.com/InterviewQuestionList.php?tech=229&dl=Top&s=Dynamic%20Programming%20Interview%20Questions%20and%20Answers#:~:text=Find%20the%20contiguous%20subarray%20with,the%20largest%20sum)). It works as follows: iterate through `nums`, keeping track of `current_max` (max subarray sum ending at the current index) and `global_max`. Initialize both as the first element (-2 in this case). Then for each next element, update `current_max = max(num, current_max + num)`. For the example: start -2; then 1 (current_max becomes max(1, -2+1)=1); then -3 (max(-3, 1+(-3))=-2); then 4 (max(4, -2+4)=4); then -1 (max(-1, 4+(-1))=3); then 2 (max(2, 3+2)=5); then 1 (max(1, 5+1)=6) – update global_max to 6; then -5 (max(-5, 6+(-5))=1); then 4 (max(4, 1+4)=5). The highest `global_max` seen was 6, which is the answer ([30 Top Dynamic Programming Interview Questions and Answers (2024)](https://www.withoutbook.com/InterviewQuestionList.php?tech=229&dl=Top&s=Dynamic%20Programming%20Interview%20Questions%20and%20Answers#:~:text=Find%20the%20contiguous%20subarray%20with,the%20largest%20sum)). Kadane’s algorithm concludes that 6 is the maximum subarray sum. Its elegance is in only using O(1) extra space and O(n) time. (If all numbers are negative, Kadane’s will correctly pick the largest single element as the max subarray.)

## Floyd’s Cycle Detection Algorithm (Tortoise & Hare)

**Definition:** *Floyd’s cycle-finding algorithm*, also known as the *Tortoise and Hare* algorithm, is a pointer algorithm that uses two pointers moving at different speeds to detect a cycle in a sequence of values ([Cycle detection - Wikipedia](https://en.wikipedia.org/wiki/Cycle_detection#:~:text=Several%20algorithms%20are%20known%20for,memory%20for%20fewer%20function%20evaluations)). Typically, one pointer (`hare`) moves two steps at a time and the other (`tortoise`) moves one step at a time. If there is a cycle (loop) in the structure being traversed, the fast pointer will eventually lap the slow pointer and they will meet; if there is no cycle, the fast pointer will reach the end (null in a linked list, or out of bounds in an array sequence).

**When to use:** Use Floyd’s algorithm whenever you need to determine if a cycle exists in a sequence generated by repeatedly applying a function or in a linked list. The classic case is detecting a loop in a singly linked list (does the linked list end in a null or does it loop back and never end?). It’s also used in algorithmic problems where an iterative process is defined (like the sequence of digits sums in the “happy number” problem) – if the process enters a cycle, Floyd’s will detect it. Another use is finding the start of the cycle: once a meeting point is found, you can reset one pointer to the head and move both one step at a time; when they meet again, it will be at the cycle’s starting node. Floyd’s algorithm is preferred for cycle detection because it uses O(1) space and O(n) time, which is very efficient.

**Example:** *Linked List Cycle Detection:* Consider a linked list: `3 → 2 → 0 → -4 → (back to 2)`. There is a cycle returning from -4 to the node with value 2. If we set `slow` and `fast` at the head (value 3), and move `fast` by 2 and `slow` by 1, the sequence of positions is: slow at 3, fast at 3 (start); slow at 2, fast at 0; slow at 0, fast at 2; slow at -4, fast at -4 – they meet, indicating a cycle. In general, if `fast` and `slow` ever point to the same node (before `fast` hits null), a cycle exists ([Coding Patterns: A Cheat Sheet](https://www.designgurus.io/course-play/grokking-the-coding-interview/doc/coding-patterns-a-cheat-sheet#:~:text=Description%3A%20In%20this%20method%2C%20two,speeds%20in%20a%20data%20structure)). If `fast` becomes null, no cycle. This algorithm can then be extended to find the entry of the cycle by resetting `slow` to head and moving both one step at a time until they meet again. Floyd’s method is also used for detecting cycles in pseudorandom number generation or functional iterations: for instance, given a function f(x), if you form the sequence x, f(x), f(f(x)), ... and want to know if it eventually cycles, running a tortoise= f(x), hare = f(f(x)) loop will determine that.

## Rabin-Karp Algorithm (String Matching)

**Definition:** *Rabin-Karp* is a string-searching algorithm that uses hashing to find an exact match of a pattern string within a text. It computes a hash value for the pattern and for substrings of the text of the same length, and then slides the window over the text, updating the hash efficiently (using a rolling hash) at each step ([Rabin–Karp algorithm - Wikipedia](https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm#:~:text=In%20computer%20science%20%2C%20the,for%20more%20than%20one%20pattern)). When a substring’s hash matches the pattern’s hash, it then checks the substring characters to confirm a match (to avoid hash collisions). The power of Rabin-Karp is that the rolling hash allows the algorithm to discard most positions in constant time and only do full comparisons occasionally.

**When to use:** Use Rabin-Karp when you need to search for a pattern in a text and want an average linear-time solution that can easily extend to multiple pattern search. It's especially useful for detecting any one of a set of patterns in a text in a single pass (multiple pattern matching) or for plagiarism detection where you might search for many snippets. Rabin-Karp is conceptually simple and works well with few or short patterns. However, its worst-case complexity can degrade (for a pathological input where all hashes match spuriously, it could become O(n*m)), so it's often used with either a good hashing technique or in expected-case scenarios. It’s also useful in 2D grid pattern searching (treat rows as big base numbers to hash sub-matrices) by extending the idea of rolling hash to two dimensions.

**Example:** *Single Pattern Search:* Suppose you want to find the substring `"aba"` in the text `"abaxabab"`. Rabin-Karp will compute a hash for `"aba"` (pattern) and the first three-letter window of text `"aba"` (which matches here immediately). The hashes match, so it does a direct string comparison and finds a match at index 0. Then it slides one character over: window `"bax"`, update the hash in O(1), which likely won’t match the pattern’s hash. It continues this way through the text. If the hash function is chosen well, comparisons are rare. In this example, it would find another hash match at the window starting index 4 (`"abab"` text, window `"aba"` at indices 4-6 matches hash, then confirm match at index 4). A more complex example is *plagiarism detection*: given a document, you can hash all substrings of length, say, 100 characters. To check another document for plagiarism, compute hashes for all length-100 substrings and see if any hash matches one from the original document ([Rabin–Karp algorithm - Wikipedia](https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm#:~:text=A%20practical%20application%20of%20the,string%20searching%20algorithms%20are%20impractical)). Matches indicate potential copied passages. Rabin-Karp efficiently filters through the text via hashing, and only when a hash matches do we compare directly, combining efficiency with exact matching.
